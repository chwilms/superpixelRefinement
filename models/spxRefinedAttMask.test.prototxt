#Modified version of the original code from Hu et al., CVPR 2017
#
#@author Hu et al.
#@author Christian Wilms
#@date 01/05/21

name: 'Single Shot Mask - ResNet 50'

layer {
  name: "data"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 1024
      dim: 1024
    }
  }
}

layer {
  name: "seg_8"
  type: "Input"
  top: "seg_8"
  input_param {
    shape {
      dim: 1
      dim: 1
      dim: 512
      dim: 512
    }
  }
}

layer {
  name: "seg_16"
  type: "Input"
  top: "seg_16"
  input_param {
    shape {
      dim: 1
      dim: 1
      dim: 512
      dim: 512
    }
  }
}

layer {
  name: "seg_24"
  type: "Input"
  top: "seg_24"
  input_param {
    shape {
      dim: 1
      dim: 1
      dim: 512
      dim: 512
    }
  }
}

layer {
  name: "seg_32"
  type: "Input"
  top: "seg_32"
  input_param {
    shape {
      dim: 1
      dim: 1
      dim: 512
      dim: 512
    }
  }
}

layer {
  name: "seg_48"
  type: "Input"
  top: "seg_48"
  input_param {
    shape {
      dim: 1
      dim: 1
      dim: 512
      dim: 512
    }
  }
}

layer {
  name: "seg_64"
  type: "Input"
  top: "seg_64"
  input_param {
    shape {
      dim: 1
      dim: 1
      dim: 512
      dim: 512
    }
  }
}

layer {
  name: "seg_96"
  type: "Input"
  top: "seg_96"
  input_param {
    shape {
      dim: 1
      dim: 1
      dim: 512
      dim: 512
    }
  }
}

layer {
  name: "seg_128"
  type: "Input"
  top: "seg_128"
  input_param {
    shape {
      dim: 1
      dim: 1
      dim: 512
      dim: 512
    }
  }
}

layer {
  name: "ResNetnConv2dnconv1n183"
  type: "Convolution"
  bottom: "data"
  top: "ResNetnConv2dnconv1n183"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    stride: 2
    pad_h: 3
    pad_w: 3
    kernel_h: 7
    kernel_w: 7
  }
}

layer {
  name: "ResNetnBatchNorm2dnbn1n184"
  type: "BatchNorm"
  bottom: "ResNetnConv2dnconv1n183"
  top: "ResNetnBatchNorm2dnbn1n184"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnBatchNorm2dnbn1n184_scale"
  type: "Scale"
  bottom: "ResNetnBatchNorm2dnbn1n184"
  top: "ResNetnBatchNorm2dnbn1n184"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnReLUnrelun185"
  type: "ReLU"
  bottom: "ResNetnBatchNorm2dnbn1n184"
  top: "ResNetnBatchNorm2dnbn1n184"
}

layer {
  name: "ResNetnMaxPool2dnmaxpooln186"
  type: "Pooling"
  bottom: "ResNetnBatchNorm2dnbn1n184"
  top: "ResNetnMaxPool2dnmaxpooln186"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn0nnConv2dnconv1n187"
  type: "Convolution"
  bottom: "ResNetnMaxPool2dnmaxpooln186"
  top: "ResNetnSequentialnlayer1nnBasicBlockn0nnConv2dnconv1n187"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn0nnBatchNorm2dnbn1n188"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn0nnConv2dnconv1n187"
  top: "ResNetnSequentialnlayer1nnBasicBlockn0nnBatchNorm2dnbn1n188"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn0nnBatchNorm2dnbn1n188_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn0nnBatchNorm2dnbn1n188"
  top: "ResNetnSequentialnlayer1nnBasicBlockn0nnBatchNorm2dnbn1n188"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn0nnReLUnrelun189"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn0nnBatchNorm2dnbn1n188"
  top: "ResNetnSequentialnlayer1nnBasicBlockn0nnBatchNorm2dnbn1n188"
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn0nnConv2dnconv2n190"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn0nnBatchNorm2dnbn1n188"
  top: "ResNetnSequentialnlayer1nnBasicBlockn0nnConv2dnconv2n190"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn0nnBatchNorm2dnbn2n191"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn0nnConv2dnconv2n190"
  top: "ResNetnSequentialnlayer1nnBasicBlockn0nnBatchNorm2dnbn2n191"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn0nnBatchNorm2dnbn2n191_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn0nnBatchNorm2dnbn2n191"
  top: "ResNetnSequentialnlayer1nnBasicBlockn0nnBatchNorm2dnbn2n191"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn0n192"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn0nnBatchNorm2dnbn2n191"
  bottom: "ResNetnMaxPool2dnmaxpooln186"
  top: "ResNetnSequentialnlayer1nnBasicBlockn0n192"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn0nnReLUnrelun193"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn0n192"
  top: "ResNetnSequentialnlayer1nnBasicBlockn0n192"
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn1nnConv2dnconv1n194"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn0n192"
  top: "ResNetnSequentialnlayer1nnBasicBlockn1nnConv2dnconv1n194"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn1nnBatchNorm2dnbn1n195"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn1nnConv2dnconv1n194"
  top: "ResNetnSequentialnlayer1nnBasicBlockn1nnBatchNorm2dnbn1n195"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn1nnBatchNorm2dnbn1n195_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn1nnBatchNorm2dnbn1n195"
  top: "ResNetnSequentialnlayer1nnBasicBlockn1nnBatchNorm2dnbn1n195"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn1nnReLUnrelun196"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn1nnBatchNorm2dnbn1n195"
  top: "ResNetnSequentialnlayer1nnBasicBlockn1nnBatchNorm2dnbn1n195"
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn1nnConv2dnconv2n197"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn1nnBatchNorm2dnbn1n195"
  top: "ResNetnSequentialnlayer1nnBasicBlockn1nnConv2dnconv2n197"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn1nnBatchNorm2dnbn2n198"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn1nnConv2dnconv2n197"
  top: "ResNetnSequentialnlayer1nnBasicBlockn1nnBatchNorm2dnbn2n198"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn1nnBatchNorm2dnbn2n198_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn1nnBatchNorm2dnbn2n198"
  top: "ResNetnSequentialnlayer1nnBasicBlockn1nnBatchNorm2dnbn2n198"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn1n199"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn1nnBatchNorm2dnbn2n198"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn0n192"
  top: "ResNetnSequentialnlayer1nnBasicBlockn1n199"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn1nnReLUnrelun200"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn1n199"
  top: "ResNetnSequentialnlayer1nnBasicBlockn1n199"
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn2nnConv2dnconv1n201"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn1n199"
  top: "ResNetnSequentialnlayer1nnBasicBlockn2nnConv2dnconv1n201"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn2nnBatchNorm2dnbn1n202"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn2nnConv2dnconv1n201"
  top: "ResNetnSequentialnlayer1nnBasicBlockn2nnBatchNorm2dnbn1n202"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn2nnBatchNorm2dnbn1n202_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn2nnBatchNorm2dnbn1n202"
  top: "ResNetnSequentialnlayer1nnBasicBlockn2nnBatchNorm2dnbn1n202"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn2nnReLUnrelun203"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn2nnBatchNorm2dnbn1n202"
  top: "ResNetnSequentialnlayer1nnBasicBlockn2nnBatchNorm2dnbn1n202"
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn2nnConv2dnconv2n204"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn2nnBatchNorm2dnbn1n202"
  top: "ResNetnSequentialnlayer1nnBasicBlockn2nnConv2dnconv2n204"
  convolution_param {
    num_output: 64
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn2nnBatchNorm2dnbn2n205"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn2nnConv2dnconv2n204"
  top: "ResNetnSequentialnlayer1nnBasicBlockn2nnBatchNorm2dnbn2n205"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn2nnBatchNorm2dnbn2n205_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn2nnBatchNorm2dnbn2n205"
  top: "ResNetnSequentialnlayer1nnBasicBlockn2nnBatchNorm2dnbn2n205"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn2n206"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn2nnBatchNorm2dnbn2n205"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn1n199"
  top: "ResNetnSequentialnlayer1nnBasicBlockn2n206"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer1nnBasicBlockn2nnReLUnrelun207"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn2n206"
  top: "ResNetnSequentialnlayer1nnBasicBlockn2n206"
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn0nnConv2dnconv1n208"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn2n206"
  top: "ResNetnSequentialnlayer2nnBasicBlockn0nnConv2dnconv1n208"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    stride: 2
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn0nnSequentialndownsamplennConv2dn0n213"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer1nnBasicBlockn2n206"
  top: "ResNetnSequentialnlayer2nnBasicBlockn0nnSequentialndownsamplennConv2dn0n213"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    stride: 2
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
  }
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn0nnBatchNorm2dnbn1n209"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn0nnConv2dnconv1n208"
  top: "ResNetnSequentialnlayer2nnBasicBlockn0nnBatchNorm2dnbn1n209"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn0nnBatchNorm2dnbn1n209_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn0nnBatchNorm2dnbn1n209"
  top: "ResNetnSequentialnlayer2nnBasicBlockn0nnBatchNorm2dnbn1n209"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n214"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn0nnSequentialndownsamplennConv2dn0n213"
  top: "ResNetnSequentialnlayer2nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n214"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n214_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n214"
  top: "ResNetnSequentialnlayer2nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n214"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn0nnReLUnrelun210"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn0nnBatchNorm2dnbn1n209"
  top: "ResNetnSequentialnlayer2nnBasicBlockn0nnBatchNorm2dnbn1n209"
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn0nnConv2dnconv2n211"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn0nnBatchNorm2dnbn1n209"
  top: "ResNetnSequentialnlayer2nnBasicBlockn0nnConv2dnconv2n211"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn0nnBatchNorm2dnbn2n212"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn0nnConv2dnconv2n211"
  top: "ResNetnSequentialnlayer2nnBasicBlockn0nnBatchNorm2dnbn2n212"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn0nnBatchNorm2dnbn2n212_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn0nnBatchNorm2dnbn2n212"
  top: "ResNetnSequentialnlayer2nnBasicBlockn0nnBatchNorm2dnbn2n212"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn0n215"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn0nnBatchNorm2dnbn2n212"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n214"
  top: "ResNetnSequentialnlayer2nnBasicBlockn0n215"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn0nnReLUnrelun216"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn0n215"
  top: "ResNetnSequentialnlayer2nnBasicBlockn0n215"
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn1nnConv2dnconv1n217"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn0n215"
  top: "ResNetnSequentialnlayer2nnBasicBlockn1nnConv2dnconv1n217"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn1nnBatchNorm2dnbn1n218"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn1nnConv2dnconv1n217"
  top: "ResNetnSequentialnlayer2nnBasicBlockn1nnBatchNorm2dnbn1n218"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn1nnBatchNorm2dnbn1n218_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn1nnBatchNorm2dnbn1n218"
  top: "ResNetnSequentialnlayer2nnBasicBlockn1nnBatchNorm2dnbn1n218"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn1nnReLUnrelun219"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn1nnBatchNorm2dnbn1n218"
  top: "ResNetnSequentialnlayer2nnBasicBlockn1nnBatchNorm2dnbn1n218"
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn1nnConv2dnconv2n220"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn1nnBatchNorm2dnbn1n218"
  top: "ResNetnSequentialnlayer2nnBasicBlockn1nnConv2dnconv2n220"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn1nnBatchNorm2dnbn2n221"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn1nnConv2dnconv2n220"
  top: "ResNetnSequentialnlayer2nnBasicBlockn1nnBatchNorm2dnbn2n221"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn1nnBatchNorm2dnbn2n221_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn1nnBatchNorm2dnbn2n221"
  top: "ResNetnSequentialnlayer2nnBasicBlockn1nnBatchNorm2dnbn2n221"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn1n222"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn1nnBatchNorm2dnbn2n221"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn0n215"
  top: "ResNetnSequentialnlayer2nnBasicBlockn1n222"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn1nnReLUnrelun223"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn1n222"
  top: "ResNetnSequentialnlayer2nnBasicBlockn1n222"
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn2nnConv2dnconv1n224"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn1n222"
  top: "ResNetnSequentialnlayer2nnBasicBlockn2nnConv2dnconv1n224"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn2nnBatchNorm2dnbn1n225"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn2nnConv2dnconv1n224"
  top: "ResNetnSequentialnlayer2nnBasicBlockn2nnBatchNorm2dnbn1n225"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn2nnBatchNorm2dnbn1n225_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn2nnBatchNorm2dnbn1n225"
  top: "ResNetnSequentialnlayer2nnBasicBlockn2nnBatchNorm2dnbn1n225"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn2nnReLUnrelun226"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn2nnBatchNorm2dnbn1n225"
  top: "ResNetnSequentialnlayer2nnBasicBlockn2nnBatchNorm2dnbn1n225"
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn2nnConv2dnconv2n227"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn2nnBatchNorm2dnbn1n225"
  top: "ResNetnSequentialnlayer2nnBasicBlockn2nnConv2dnconv2n227"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn2nnBatchNorm2dnbn2n228"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn2nnConv2dnconv2n227"
  top: "ResNetnSequentialnlayer2nnBasicBlockn2nnBatchNorm2dnbn2n228"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn2nnBatchNorm2dnbn2n228_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn2nnBatchNorm2dnbn2n228"
  top: "ResNetnSequentialnlayer2nnBasicBlockn2nnBatchNorm2dnbn2n228"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn2n229"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn2nnBatchNorm2dnbn2n228"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn1n222"
  top: "ResNetnSequentialnlayer2nnBasicBlockn2n229"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn2nnReLUnrelun230"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn2n229"
  top: "ResNetnSequentialnlayer2nnBasicBlockn2n229"
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn3nnConv2dnconv1n231"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn2n229"
  top: "ResNetnSequentialnlayer2nnBasicBlockn3nnConv2dnconv1n231"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn3nnBatchNorm2dnbn1n232"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn3nnConv2dnconv1n231"
  top: "ResNetnSequentialnlayer2nnBasicBlockn3nnBatchNorm2dnbn1n232"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn3nnBatchNorm2dnbn1n232_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn3nnBatchNorm2dnbn1n232"
  top: "ResNetnSequentialnlayer2nnBasicBlockn3nnBatchNorm2dnbn1n232"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn3nnReLUnrelun233"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn3nnBatchNorm2dnbn1n232"
  top: "ResNetnSequentialnlayer2nnBasicBlockn3nnBatchNorm2dnbn1n232"
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn3nnConv2dnconv2n234"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn3nnBatchNorm2dnbn1n232"
  top: "ResNetnSequentialnlayer2nnBasicBlockn3nnConv2dnconv2n234"
  convolution_param {
    num_output: 128
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn3nnBatchNorm2dnbn2n235"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn3nnConv2dnconv2n234"
  top: "ResNetnSequentialnlayer2nnBasicBlockn3nnBatchNorm2dnbn2n235"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn3nnBatchNorm2dnbn2n235_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn3nnBatchNorm2dnbn2n235"
  top: "ResNetnSequentialnlayer2nnBasicBlockn3nnBatchNorm2dnbn2n235"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn3n236"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn3nnBatchNorm2dnbn2n235"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn2n229"
  top: "ResNetnSequentialnlayer2nnBasicBlockn3n236"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer2nnBasicBlockn3nnReLUnrelun237"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn3n236"
  top: "ResNetnSequentialnlayer2nnBasicBlockn3n236"
}

####################### div2 ###########################


layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnConv2dnconv1n238"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn3n236"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnConv2dnconv1n238"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 2
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennConv2dn0n243"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn3n236"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennConv2dn0n243"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 2
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn1n239"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnConv2dnconv1n238"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn1n239"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn1n239_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn1n239"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn1n239"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n244"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennConv2dn0n243"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n244"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n244_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n244"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n244"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnReLUnrelun240"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn1n239"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn1n239"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnConv2dnconv2n241"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn1n239"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnConv2dnconv2n241"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn2n242"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnConv2dnconv2n241"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn2n242"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn2n242_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn2n242"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn2n242"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0n245"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn2n242"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n244"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0n245"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnReLUnrelun246"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0n245"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0n245"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1nnConv2dnconv1n247"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0n245"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1nnConv2dnconv1n247"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn1n248"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1nnConv2dnconv1n247"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn1n248"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn1n248_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn1n248"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn1n248"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1nnReLUnrelun249"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn1n248"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn1n248"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1nnConv2dnconv2n250"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn1n248"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1nnConv2dnconv2n250"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn2n251"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1nnConv2dnconv2n250"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn2n251"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn2n251_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn2n251"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn2n251"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1n252"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn2n251"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0n245"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1n252"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1nnReLUnrelun253"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1n252"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1n252"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2nnConv2dnconv1n254"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1n252"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2nnConv2dnconv1n254"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn1n255"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2nnConv2dnconv1n254"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn1n255"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn1n255_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn1n255"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn1n255"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2nnReLUnrelun256"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn1n255"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn1n255"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2nnConv2dnconv2n257"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn1n255"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2nnConv2dnconv2n257"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn2n258"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2nnConv2dnconv2n257"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn2n258"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn2n258_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn2n258"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn2n258"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2n259"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn2n258"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1n252"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2n259"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2nnReLUnrelun260"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2n259"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2n259"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3nnConv2dnconv1n261"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2n259"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3nnConv2dnconv1n261"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn1n262"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3nnConv2dnconv1n261"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn1n262"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn1n262_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn1n262"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn1n262"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3nnReLUnrelun263"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn1n262"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn1n262"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3nnConv2dnconv2n264"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn1n262"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3nnConv2dnconv2n264"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn2n265"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3nnConv2dnconv2n264"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn2n265"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn2n265_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn2n265"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn2n265"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3n266"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn2n265"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2n259"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3n266"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3nnReLUnrelun267"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3n266"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3n266"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4nnConv2dnconv1n268"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3n266"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4nnConv2dnconv1n268"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn1n269"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4nnConv2dnconv1n268"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn1n269"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn1n269_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn1n269"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn1n269"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4nnReLUnrelun270"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn1n269"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn1n269"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4nnConv2dnconv2n271"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn1n269"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4nnConv2dnconv2n271"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn2n272"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4nnConv2dnconv2n271"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn2n272"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn2n272_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn2n272"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn2n272"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4n273"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn2n272"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3n266"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4n273"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4nnReLUnrelun274"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4n273"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4n273"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5nnConv2dnconv1n275"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4n273"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5nnConv2dnconv1n275"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn1n276"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5nnConv2dnconv1n275"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn1n276"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn1n276_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn1n276"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn1n276"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5nnReLUnrelun277"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn1n276"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn1n276"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5nnConv2dnconv2n278"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn1n276"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5nnConv2dnconv2n278"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn2n279"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5nnConv2dnconv2n278"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn2n279"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn2n279_scale"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn2n279"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn2n279"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5n280"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn2n279"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4n273"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5n280"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5nnReLUnrelun281"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5n280"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5n280"
}


####################### div2 ###########################


layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnConv2dnconv1n238_div3"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn3n236"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnConv2dnconv1n238_div3"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 3
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennConv2dn0n243_div3"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer2nnBasicBlockn3n236"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennConv2dn0n243_div3"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 3
    pad_h: 0
    pad_w: 0
    kernel_h: 1
    kernel_w: 1
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn1n239_div3"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnConv2dnconv1n238_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn1n239_div3"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn1n239_scale_div3"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn1n239_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn1n239_div3"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n244_div3"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennConv2dn0n243_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n244_div3"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n244_scale_div3"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n244_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n244_div3"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnReLUnrelun240_div3"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn1n239_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn1n239_div3"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnConv2dnconv2n241_div3"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn1n239_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnConv2dnconv2n241_div3"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn2n242_div3"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnConv2dnconv2n241_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn2n242_div3"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn2n242_scale_div3"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn2n242_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn2n242_div3"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0n245_div3"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnBatchNorm2dnbn2n242_div3"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0nnSequentialndownsamplennBatchNorm2dn1n244_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0n245_div3"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn0nnReLUnrelun246_div3"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0n245_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn0n245_div3"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1nnConv2dnconv1n247_div3"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0n245_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1nnConv2dnconv1n247_div3"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn1n248_div3"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1nnConv2dnconv1n247_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn1n248_div3"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn1n248_scale_div3"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn1n248_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn1n248_div3"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1nnReLUnrelun249_div3"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn1n248_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn1n248_div3"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1nnConv2dnconv2n250_div3"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn1n248_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1nnConv2dnconv2n250_div3"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn2n251_div3"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1nnConv2dnconv2n250_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn2n251_div3"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn2n251_scale_div3"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn2n251_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn2n251_div3"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1n252_div3"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1nnBatchNorm2dnbn2n251_div3"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn0n245_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1n252_div3"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn1nnReLUnrelun253_div3"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1n252_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn1n252_div3"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2nnConv2dnconv1n254_div3"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1n252_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2nnConv2dnconv1n254_div3"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn1n255_div3"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2nnConv2dnconv1n254_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn1n255_div3"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn1n255_scale_div3"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn1n255_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn1n255_div3"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2nnReLUnrelun256_div3"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn1n255_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn1n255_div3"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2nnConv2dnconv2n257_div3"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn1n255_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2nnConv2dnconv2n257_div3"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn2n258_div3"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2nnConv2dnconv2n257_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn2n258_div3"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn2n258_scale_div3"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn2n258_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn2n258_div3"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2n259_div3"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2nnBatchNorm2dnbn2n258_div3"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn1n252_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2n259_div3"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn2nnReLUnrelun260_div3"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2n259_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn2n259_div3"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3nnConv2dnconv1n261_div3"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2n259_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3nnConv2dnconv1n261_div3"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn1n262_div3"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3nnConv2dnconv1n261_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn1n262_div3"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn1n262_scale_div3"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn1n262_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn1n262_div3"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3nnReLUnrelun263_div3"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn1n262_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn1n262_div3"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3nnConv2dnconv2n264_div3"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn1n262_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3nnConv2dnconv2n264_div3"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn2n265_div3"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3nnConv2dnconv2n264_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn2n265_div3"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn2n265_scale_div3"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn2n265_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn2n265_div3"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3n266_div3"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3nnBatchNorm2dnbn2n265_div3"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn2n259_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3n266_div3"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn3nnReLUnrelun267_div3"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3n266_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn3n266_div3"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4nnConv2dnconv1n268_div3"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3n266_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4nnConv2dnconv1n268_div3"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn1n269_div3"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4nnConv2dnconv1n268_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn1n269_div3"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn1n269_scale_div3"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn1n269_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn1n269_div3"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4nnReLUnrelun270_div3"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn1n269_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn1n269_div3"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4nnConv2dnconv2n271_div3"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn1n269_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4nnConv2dnconv2n271_div3"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn2n272_div3"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4nnConv2dnconv2n271_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn2n272_div3"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn2n272_scale_div3"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn2n272_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn2n272_div3"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4n273_div3"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4nnBatchNorm2dnbn2n272_div3"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn3n266_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4n273_div3"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn4nnReLUnrelun274_div3"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4n273_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn4n273_div3"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5nnConv2dnconv1n275_div3"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4n273_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5nnConv2dnconv1n275_div3"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn1n276_div3"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5nnConv2dnconv1n275_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn1n276_div3"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn1n276_scale_div3"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn1n276_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn1n276_div3"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5nnReLUnrelun277_div3"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn1n276_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn1n276_div3"
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5nnConv2dnconv2n278_div3"
  type: "Convolution"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn1n276_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5nnConv2dnconv2n278_div3"
  convolution_param {
    num_output: 256
    bias_term: false
    group: 1
    stride: 1
    pad_h: 1
    pad_w: 1
    kernel_h: 3
    kernel_w: 3
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn2n279_div3"
  type: "BatchNorm"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5nnConv2dnconv2n278_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn2n279_div3"
  batch_norm_param {
        use_global_stats: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn2n279_scale_div3"
  type: "Scale"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn2n279_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn2n279_div3"
  scale_param {
        bias_term: true
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
    }
    param {
        lr_mult: 0.0
        decay_mult: 0.0
}
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5n280_div3"
  type: "Eltwise"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5nnBatchNorm2dnbn2n279_div3"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn4n273_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5n280_div3"
  eltwise_param {
    operation: SUM
  }
}

layer {
  name: "ResNetnSequentialnlayer3nnBasicBlockn5nnReLUnrelun281_div3"
  type: "ReLU"
  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5n280_div3"
  top: "ResNetnSequentialnlayer3nnBasicBlockn5n280_div3"
}

########## shared deep_mask feature on VGG16 48s ##########

layer { name: "conv5_4_div3" type:"Convolution" bottom: "ResNetnSequentialnlayer3nnBasicBlockn5n280_div3" top: "conv5_4_div3"
  param { name: 'rnn_conv_1_weight' lr_mult: 10} param { name: 'rnn_conv_1_bias' lr_mult: 20}
  convolution_param { kernel_size: 3 stride: 1 pad: 1  num_output: 256
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "conv5_4_div3_relu" type: "ReLU" bottom: "conv5_4_div3" top: "conv5_4_div3" }

layer { name: "conv5_5_div3" type: "Convolution" bottom: "conv5_4_div3" top: "conv5_5_div3"
  param { name: 'rnn_conv_2_weight' lr_mult: 10} param { name: 'rnn_conv_2_bias' lr_mult: 20}
  convolution_param { kernel_size: 1 stride: 1 pad: 0  num_output: 256
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "pool5b_div3" type: "Pooling" bottom: "conv5_5_div3" top: "pool5b_div3" 
  pooling_param { pool: AVE kernel_size: 2 stride: 2} }

layer { name: "pool5a_div3" type: "Pooling" bottom: "ResNetnSequentialnlayer3nnBasicBlockn5n280_div3" top: "pool5a_div3" 
  pooling_param { pool: AVE kernel_size: 2 stride: 2} }

layer { name: "sum_48s" type: "Eltwise" bottom: "pool5a_div3" bottom: "pool5b_div3" top: "sum_48s" }
layer { name: "sum_48s_relu" type: "ReLU" bottom: "sum_48s" top: "sum_48s" }


########## shared deep_mask feature on VGG16 96s ##########

layer { name: "conv6_4_div3" type:"Convolution" bottom: "sum_48s" top: "conv6_4_div3"
  param { name: 'rnn_conv_1_weight' lr_mult: 10} param { name: 'rnn_conv_1_bias' lr_mult: 20}
  convolution_param { kernel_size: 3 stride: 1 pad: 1  num_output: 256
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "conv6_4_div3_relu" type: "ReLU" bottom: "conv6_4_div3" top: "conv6_4_div3" }

layer { name: "conv6_5_div3" type:"Convolution" bottom: "conv6_4_div3" top: "conv6_5_div3"
  param { name: 'rnn_conv_2_weight' lr_mult: 10} param { name: 'rnn_conv_2_bias' lr_mult: 20}
  convolution_param { kernel_size: 1 stride: 1 pad: 0  num_output: 256
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "pool6b_div3" type:"Pooling" bottom: "conv6_5_div3" top: "pool6b_div3"
  pooling_param { pool: AVE kernel_size: 2 stride: 2 } }

layer { name: "pool6a_div3" type:"Pooling" bottom: "sum_48s" top: "pool6a_div3"
  pooling_param { pool: AVE kernel_size: 2 stride: 2 } }

layer { name: "sum_96s" type: "Eltwise" bottom: "pool6a_div3" bottom: "pool6b_div3" top: "sum_96s" }
layer { name: "sum_96s_relu" type: "ReLU" bottom: "sum_96s" top: "sum_96s" }


########## shared neck at scale 32 ##########

layer { name: "conv5_4" type:"Convolution" bottom: "ResNetnSequentialnlayer3nnBasicBlockn5n280" top: "conv5_4"
  param { name: 'rnn_conv_1_weight' } param { name: 'rnn_conv_1_bias' }
  convolution_param { kernel_size: 3 stride: 1 pad: 1  num_output: 256
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "conv5_4_relu" type: "ReLU" bottom: "conv5_4" top: "conv5_4" }

layer { name: "conv5_5" type: "Convolution" bottom: "conv5_4" top: "conv5_5"
  param { name: 'rnn_conv_2_weight' } param { name: 'rnn_conv_2_bias' }
  convolution_param { kernel_size: 1 stride: 1 pad: 0  num_output: 256
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "pool5b" type: "Pooling" bottom: "conv5_5" top: "pool5b"
  pooling_param { pool: AVE kernel_size: 2 stride: 2} }

layer { name: "pool5a" type: "Pooling" bottom: "ResNetnSequentialnlayer3nnBasicBlockn5n280" top: "pool5a"
  pooling_param { pool: AVE kernel_size: 2 stride: 2} }

layer { name: "sum_32s" type: "Eltwise" bottom: "pool5a" bottom: "pool5b" top: "sum_32s" }
layer { name: "sum_32s_relu" type: "ReLU" bottom: "sum_32s" top: "sum_32s" }

########## shared neck at scale 64 ##########

layer { name: "conv6_4" type:"Convolution" bottom: "sum_32s" top: "conv6_4"
  param { name: 'rnn_conv_1_weight' } param { name: 'rnn_conv_1_bias' }
  convolution_param { kernel_size: 3 stride: 1 pad: 1  num_output: 256
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "conv6_4_relu" type: "ReLU" bottom: "conv6_4" top: "conv6_4" }

layer { name: "conv6_5" type: "Convolution" bottom: "conv6_4" top: "conv6_5"
  param { name: 'rnn_conv_2_weight' } param { name: 'rnn_conv_2_bias' }
  convolution_param { kernel_size: 1 stride: 1 pad: 0  num_output: 256
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "pool6b" type: "Pooling" bottom: "conv6_5" top: "pool6b"
  pooling_param { pool: AVE kernel_size: 2 stride: 2} }

layer { name: "pool6a" type: "Pooling" bottom: "sum_32s" top: "pool6a"
  pooling_param { pool: AVE kernel_size: 2 stride: 2} }

layer { name: "sum_64s" type: "Eltwise" bottom: "pool6a" bottom: "pool6b" top: "sum_64s" }
layer { name: "sum_64s_relu" type: "ReLU" bottom: "sum_64s" top: "sum_64s" }


########## shared neck at scale 128 ##########

layer { name: "conv7_4" type:"Convolution" bottom: "sum_64s" top: "conv7_4"
  param { name: 'rnn_conv_1_weight' } param { name: 'rnn_conv_1_bias' }
  convolution_param { kernel_size: 3 stride: 1 pad: 1  num_output: 256
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "conv7_4_relu" type: "ReLU" bottom: "conv7_4" top: "conv7_4" }

layer { name: "conv7_5" type: "Convolution" bottom: "conv7_4" top: "conv7_5"
  param { name: 'rnn_conv_2_weight' } param { name: 'rnn_conv_2_bias' }
  convolution_param { kernel_size: 1 stride: 1 pad: 0  num_output: 256
    weight_filler { type: 'gaussian' std: 0.00001 } bias_filler { type: 'constant' value: 0 } } }

layer { name: "pool7b" type: "Pooling" bottom: "conv7_5" top: "pool7b"
  pooling_param { pool: AVE kernel_size: 2 stride: 2} }

layer { name: "pool7a" type: "Pooling" bottom: "sum_64s" top: "pool7a"
  pooling_param { pool: AVE kernel_size: 2 stride: 2} }

layer { name: "sum_128s" type: "Eltwise" bottom: "pool7a" bottom: "pool7b" top: "sum_128s" }
layer { name: "sum_128s_relu" type: "ReLU" bottom: "sum_128s" top: "sum_128s" }


########## objectness attention at scale 8 ##########

layer {  name: "res_buAttSize1a_8"  type: "Convolution"  bottom: "ResNetnSequentialnlayer2nnBasicBlockn3n236"  top: "res_buAttSize1a_8"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 128  pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }  }
}

layer { name: "relu_buAttSize1a_8" type: "ReLU" bottom: "res_buAttSize1a_8" top: "res_buAttSize1a_8" }

layer {  name: "res_buAttSize1b_8"  type: "Convolution"  bottom: "res_buAttSize1a_8"  top: "res_buAttSize1b_8"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 2  pad: 2    kernel_size: 4    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: .5    }  }
}

layer { name: "loss_buAttSize_8" type: "Softmax" bottom: "res_buAttSize1b_8" top: "finalAtt8" }

layer {
  name: "slicer_label"
  type: "Slice"
  bottom: "finalAtt8"
  top: "nonObj8"
  top: "obj8"
  slice_param {
    axis: 1
    slice_point: 1
  }
}

layer{
  type: "Reshape"
  bottom: "obj8"
  top: "obj8_flat"
  reshape_param {
      shape {
        dim: 1  
        dim: 1
        dim: 1
        dim: -1 
      }
    }
}

layer{
  type: "Reshape"
  bottom: "nonObj8"
  top: "nonObj8_flat"
  reshape_param {
      shape {
        dim: 1  
        dim: 1
        dim: 1
        dim: -1 
      }
    }
}

layer{
  type: "CheckAtt"
  bottom: "obj8_flat"
  bottom: "nonObj8_flat"
  top: "obj8_checked"
  top: "nonObj8_checked"
}


########## objectness attention at scale 16 ##########

layer {  name: "res_buAttSize1a_16"  type: "Convolution"  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5n280"  top: "res_buAttSize1a_16"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 128  pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }  }
}

layer { name: "relu_buAttSize1a_16" type: "ReLU" bottom: "res_buAttSize1a_16" top: "res_buAttSize1a_16" }

layer {  name: "res_buAttSize1b_16"  type: "Convolution"  bottom: "res_buAttSize1a_16"  top: "res_buAttSize1b_16"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 2  pad: 2    kernel_size: 4    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: .5    }  }
}

layer { name: "loss_buAttSize_16" type: "Softmax" bottom: "res_buAttSize1b_16" top: "finalAtt16" }

layer {
  name: "slicer_label"
  type: "Slice"
  bottom: "finalAtt16"
  top: "nonObj16"
  top: "obj16"
  slice_param {
    axis: 1
    slice_point: 1
  }
}

layer{
  type: "Reshape"
  bottom: "obj16"
  top: "obj16_flat"
  reshape_param {
      shape {
        dim: 1  
        dim: 1
        dim: 1
        dim: -1 
      }
    }
}

layer{
  type: "Reshape"
  bottom: "nonObj16"
  top: "nonObj16_flat"
  reshape_param {
      shape {
        dim: 1  
        dim: 1
        dim: 1
        dim: -1 
      }
    }
}

layer{
  type: "CheckAtt"
  bottom: "obj16_flat"
  bottom: "nonObj16_flat"
  top: "obj16_checked"
  top: "nonObj16_checked"
}

########## objectness attention at scale 24 ##########

layer {  name: "res_buAttSize1a_24"  type: "Convolution"  bottom: "ResNetnSequentialnlayer3nnBasicBlockn5n280_div3"  top: "res_buAttSize1a_24"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 128  pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }  }
}

layer { name: "relu_buAttSize1a_24" type: "ReLU" bottom: "res_buAttSize1a_24" top: "res_buAttSize1a_24" }

layer {  name: "res_buAttSize1b_24"  type: "Convolution"  bottom: "res_buAttSize1a_24"  top: "res_buAttSize1b_24"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 2  pad: 2    kernel_size: 4    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: .5    }  }
}

layer { name: "loss_buAttSize_24" type: "Softmax" bottom: "res_buAttSize1b_24" top: "finalAtt24" }

layer {
  name: "slicer_label"
  type: "Slice"
  bottom: "finalAtt24"
  top: "nonObj24"
  top: "obj24"
  slice_param {
    axis: 1
    slice_point: 1
  }
}

layer{
  type: "Reshape"
  bottom: "obj24"
  top: "obj24_flat"
  reshape_param {
      shape {
        dim: 1  
        dim: 1
        dim: 1
        dim: -1 
      }
    }
}

layer{
  type: "Reshape"
  bottom: "nonObj24"
  top: "nonObj24_flat"
  reshape_param {
      shape {
        dim: 1  
        dim: 1
        dim: 1
        dim: -1 
      }
    }
}

layer{
  type: "CheckAtt"
  bottom: "obj24_flat"
  bottom: "nonObj24_flat"
  top: "obj24_checked"
  top: "nonObj24_checked"
}


########## objectness attention at scale 32 ##########

layer {  name: "res_buAttSize1a_32"  type: "Convolution"  bottom: "sum_32s"  top: "res_buAttSize1a_32"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 128  pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }  }
} 

layer { name: "relu_buAttSize1a_32" type: "ReLU" bottom: "res_buAttSize1a_32" top: "res_buAttSize1a_32" }

layer {  name: "res_buAttSize1b_32"  type: "Convolution"  bottom: "res_buAttSize1a_32"  top: "res_buAttSize1b_32"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 2  pad: 2    kernel_size: 4    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: .5    }  }
}

layer { name: "loss_buAttSize_32" type: "Softmax" bottom: "res_buAttSize1b_32" top: "finalAtt32" }

layer {
  name: "slicer_label"
  type: "Slice"
  bottom: "finalAtt32"
  top: "nonObj32"
  top: "obj32"
  slice_param {
    axis: 1
    slice_point: 1
  }
}

layer{
  type: "Reshape"
  bottom: "obj32"
  top: "obj32_flat"
  reshape_param {
      shape {
        dim: 1  
        dim: 1
        dim: 1
        dim: -1 
      }
    }
}

layer{
  type: "Reshape"
  bottom: "nonObj32"
  top: "nonObj32_flat"
  reshape_param {
      shape {
        dim: 1  
        dim: 1
        dim: 1
        dim: -1 
      }
    }
}

layer{
  type: "CheckAtt"
  bottom: "obj32_flat"
  bottom: "nonObj32_flat"
  top: "obj32_checked"
  top: "nonObj32_checked"
}


########## objectness attention at scale 48 ##########

layer {  name: "res_buAttSize1a_48"  type: "Convolution"  bottom: "sum_48s"  top: "res_buAttSize1a_48"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 128  pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }  }
} 

layer { name: "relu_buAttSize1a_48" type: "ReLU" bottom: "res_buAttSize1a_48" top: "res_buAttSize1a_48" }

layer {  name: "res_buAttSize1b_48"  type: "Convolution"  bottom: "res_buAttSize1a_48"  top: "res_buAttSize1b_48"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 2  pad: 2    kernel_size: 4    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: .5    }  }
}

layer { name: "loss_buAttSize_48" type: "Softmax" bottom: "res_buAttSize1b_48" top: "finalAtt48" }

layer {
  name: "slicer_label"
  type: "Slice"
  bottom: "finalAtt48"
  top: "nonObj48"
  top: "obj48"
  slice_param {
    axis: 1
    slice_point: 1
  }
}

layer{
  type: "Reshape"
  bottom: "obj48"
  top: "obj48_flat"
  reshape_param {
      shape {
        dim: 1  
        dim: 1
        dim: 1
        dim: -1 
      }
    }
}

layer{
  type: "Reshape"
  bottom: "nonObj48"
  top: "nonObj48_flat"
  reshape_param {
      shape {
        dim: 1  
        dim: 1
        dim: 1
        dim: -1 
      }
    }
}

layer{
  type: "CheckAtt"
  bottom: "obj48_flat"
  bottom: "nonObj48_flat"
  top: "obj48_checked"
  top: "nonObj48_checked"
}



########## objectness attention at scale 64 ##########

layer {  name: "res_buAttSize1a_64"  type: "Convolution"  bottom: "sum_64s"  top: "res_buAttSize1a_64"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 128  pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }  }
} 

layer { name: "relu_buAttSize1a_64" type: "ReLU" bottom: "res_buAttSize1a_64" top: "res_buAttSize1a_64" }

layer {  name: "res_buAttSize1b_64"  type: "Convolution"  bottom: "res_buAttSize1a_64"  top: "res_buAttSize1b_64"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 2  pad: 2    kernel_size: 4    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: .5    }  }
}

layer { name: "loss_buAttSize_64" type: "Softmax" bottom: "res_buAttSize1b_64" top: "finalAtt64" }

layer {
  name: "slicer_label"
  type: "Slice"
  bottom: "finalAtt64"
  top: "nonObj64"
  top: "obj64"
  slice_param {
    axis: 1
    slice_point: 1
  }
}

layer{
  type: "Reshape"
  bottom: "obj64"
  top: "obj64_flat"
  reshape_param {
      shape {
        dim: 1  
        dim: 1
        dim: 1
        dim: -1 
      }
    }
}

layer{
  type: "Reshape"
  bottom: "nonObj64"
  top: "nonObj64_flat"
  reshape_param {
      shape {
        dim: 1  
        dim: 1
        dim: 1
        dim: -1 
      }
    }
}

layer{
  type: "CheckAtt"
  bottom: "obj64_flat"
  bottom: "nonObj64_flat"
  top: "obj64_checked"
  top: "nonObj64_checked"
}


########## objectness attention at scale 96 ##########

layer {  name: "res_buAttSize1a_96"  type: "Convolution"  bottom: "sum_96s"  top: "res_buAttSize1a_96"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 128  pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }  }
} 

layer { name: "relu_buAttSize1a_96" type: "ReLU" bottom: "res_buAttSize1a_96" top: "res_buAttSize1a_96" }

layer {  name: "res_buAttSize1b_96"  type: "Convolution"  bottom: "res_buAttSize1a_96"  top: "res_buAttSize1b_96"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 2  pad: 2    kernel_size: 4    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: .5    }  }
}

layer { name: "loss_buAttSize_96" type: "Softmax" bottom: "res_buAttSize1b_96" top: "finalAtt96" }

layer {
  name: "slicer_label"
  type: "Slice"
  bottom: "finalAtt96"
  top: "nonObj96"
  top: "obj96"
  slice_param {
    axis: 1
    slice_point: 1
  }
}

layer{
  type: "Reshape"
  bottom: "obj96"
  top: "obj96_flat"
  reshape_param {
      shape {
        dim: 1  
        dim: 1
        dim: 1
        dim: -1 
      }
    }
}

layer{
  type: "Reshape"
  bottom: "nonObj96"
  top: "nonObj96_flat"
  reshape_param {
      shape {
        dim: 1  
        dim: 1
        dim: 1
        dim: -1 
      }
    }
}

layer{
  type: "CheckAtt"
  bottom: "obj96_flat"
  bottom: "nonObj96_flat"
  top: "obj96_checked"
  top: "nonObj96_checked"
}


########## objectness attention at scale 128 ##########

layer {  name: "res_buAttSize1a_128"  type: "Convolution"  bottom: "sum_128s"  top: "res_buAttSize1a_128"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 128  pad: 1    kernel_size: 3    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: 0    }  }
} 

layer { name: "relu_buAttSize1a_128" type: "ReLU" bottom: "res_buAttSize1a_128" top: "res_buAttSize1a_128" }

layer {  name: "res_buAttSize1b_128"  type: "Convolution"  bottom: "res_buAttSize1a_128"  top: "res_buAttSize1b_128"
  param {    lr_mult: 10    decay_mult: 1  }  param {    lr_mult: 20    decay_mult: 0  }
  convolution_param {  num_output: 2  pad: 2    kernel_size: 4    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_filler {      type: "constant"      value: .5    }  }
}

layer { name: "loss_buAttSize_128" type: "Softmax" bottom: "res_buAttSize1b_128" top: "finalAtt128" }

layer {
  name: "slicer_label"
  type: "Slice"
  bottom: "finalAtt128"
  top: "nonObj128"
  top: "obj128"
  slice_param {
    axis: 1
    slice_point: 1
  }
}

layer{
  type: "Reshape"
  bottom: "obj128"
  top: "obj128_flat"
  reshape_param {
      shape {
        dim: 1  
        dim: 1
        dim: 1
        dim: -1 
      }
    }
}

layer{
  type: "Reshape"
  bottom: "nonObj128"
  top: "nonObj128_flat"
  reshape_param {
      shape {
        dim: 1  
        dim: 1
        dim: 1
        dim: -1 
      }
    }
}

layer{
  type: "CheckAtt"
  bottom: "obj128_flat"
  bottom: "nonObj128_flat"
  top: "obj128_checked"
  top: "nonObj128_checked"
}


########## BoxSelection ##########

layer {
  name: "concatFlattendObj"
  bottom: "obj8_checked"
  bottom: "obj16_checked"
  bottom: "obj24_checked"
  bottom: "obj32_checked"
  bottom: "obj48_checked"
  bottom: "obj64_checked"
  bottom: "obj96_checked"
  bottom: "obj128_checked"
  top: "flattenedObj"
  type: "Concat"
  concat_param {
    axis: -1
  }
}

layer {
  name: "concatFlattendNonObj"
  bottom: "nonObj8_checked"
  bottom: "nonObj16_checked"
  bottom: "nonObj24_checked"
  bottom: "nonObj32_checked"
  bottom: "nonObj48_checked"
  bottom: "nonObj64_checked"
  bottom: "nonObj96_checked"
  bottom: "nonObj128_checked"
  top: "flattenedNonObj"
  type: "Concat"
  concat_param {
    axis: -1
  }
}

layer {
  name:"argmax"
  type: "ArgMaxAll"
  bottom: "flattenedObj"
  bottom: "flattenedNonObj"
  top: "obj_argmax"
  argmax_param {
    top_k: 1000 #get best 1000 locations
    axis: -1
  }
}

layer {
  name:"flags"
  type: "ConvertIndices"
  bottom: "obj_argmax"
  bottom: "flattenedObj"
  top: "obj_flags"
  top: "objn_new"
}

layer {
  name: "flagsToIndices"
  type: "FlagsToIndices"
  bottom: "obj_flags"
  top: "obj_indices"
}

########## split up indices ##########

layer{
  type: "SplitIndices"
  bottom: "obj_flags"
  bottom: "obj8_checked"
  bottom: "obj16_checked"
  bottom: "obj24_checked"
  bottom: "obj32_checked"
  bottom: "obj48_checked"
  bottom: "obj64_checked"
  bottom: "obj96_checked"
  bottom: "obj128_checked"
  top: "obj8_flags"
  top: "obj16_flags"
  top: "obj24_flags"
  top: "obj32_flags"
  top: "obj48_flags"
  top: "obj64_flags"
  top: "obj96_flags"
  top: "obj128_flags"
}

########## sample windows ##########

layer { name: "conv_feat_1_8s" type: "Convolution" bottom: "ResNetnSequentialnlayer2nnBasicBlockn3n236" top: "conv_feat_1_8s"
  param { lr_mult: 1 } 
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false } }

layer { name: "extractor_8" type: "SlidingWindowIndex" bottom: "conv_feat_1_8s" bottom: "obj8_flags" top: "sample_8s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "conv_feat_1_16s" type: "Convolution" bottom: "ResNetnSequentialnlayer3nnBasicBlockn5n280" top: "conv_feat_1_16s"
  param { name: "conv_feat_1" lr_mult: 1 } 
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false } }

layer { name: "extractor_16" type: "SlidingWindowIndex" bottom: "conv_feat_1_16s" bottom: "obj16_flags" top: "sample_16s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "conv_feat_1_24s" type: "Convolution" bottom: "ResNetnSequentialnlayer3nnBasicBlockn5n280_div3" top: "conv_feat_1_24s"
  param { name: "conv_feat_1" lr_mult: 1 } 
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false } }

layer { name: "extractor_24" type: "SlidingWindowIndex" bottom: "conv_feat_1_24s" bottom: "obj24_flags" top: "sample_24s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "conv_feat_1_32s" type: "Convolution" bottom: "sum_32s" top: "conv_feat_1_32s"
  param { name: "conv_feat_1" lr_mult: 1 }
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false } }

layer { name: "extractor_32" type: "SlidingWindowIndex" bottom: "conv_feat_1_32s" bottom: "obj32_flags" top: "sample_32s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "conv_feat_1_48s" type: "Convolution" bottom: "sum_48s" top: "conv_feat_1_48s"
  param { name: "conv_feat_1" lr_mult: 1 }
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false } }

layer { name: "extractor_48" type: "SlidingWindowIndex" bottom: "conv_feat_1_48s" bottom: "obj48_flags" top: "sample_48s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "conv_feat_1_64s" type: "Convolution" bottom: "sum_64s" top: "conv_feat_1_64s"
  param { name: "conv_feat_1" lr_mult: 1 }
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false } }

layer { name: "extractor_64" type: "SlidingWindowIndex" bottom: "conv_feat_1_64s" bottom: "obj64_flags" top: "sample_64s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "conv_feat_1_96s" type: "Convolution" bottom: "sum_96s" top: "conv_feat_1_96s"
  param { name: "conv_feat_1" lr_mult: 1 }
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false } }

layer { name: "extractor_96" type: "SlidingWindowIndex" bottom: "conv_feat_1_96s" bottom: "obj96_flags" top: "sample_96s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "conv_feat_1_128s" type: "Convolution" bottom: "sum_128s" top: "conv_feat_1_128s"
  param { name: "conv_feat_1" lr_mult: 1 }
  convolution_param { num_output: 128 pad: 0 kernel_size: 1 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false } }

layer { name: "extractor_128" type: "SlidingWindowIndex" bottom: "conv_feat_1_128s" bottom: "obj128_flags" top: "sample_128s"
  sliding_window_param { window_h: 10 window_w: 10 }}

layer { name: "sample_concat" type: "Concat"
  bottom: "sample_8s" bottom: "sample_16s" bottom: "sample_24s" bottom: "sample_32s" bottom: "sample_48s" 
  bottom: "sample_64s" bottom: "sample_96s" bottom: "sample_128s" 
top: "sample"
  concat_param { concat_dim: 0 } }

layer { bottom: "sample" top: "sample_bn" type: "BatchNorm" name:"sample_bn" batch_norm_param { use_global_stats: false } 
  param { lr_mult: 0 decay_mult: 0 } param { lr_mult: 0 decay_mult: 0 } param { lr_mult: 0 decay_mult: 0 } }
layer { bottom: "sample_bn" top: "sample_s" type: "Scale" name: "sample_scale" scale_param { bias_term: true } 
  param { lr_mult: 1 decay_mult: 0 } param { lr_mult: 1 decay_mult: 0} }

########## cls branch ##########

layer { name: 'cls_1' type: 'InnerProduct' bottom: 'sample_s' top: 'cls_1'
    param { lr_mult: 1.0 } param { lr_mult: 1.0 }
    inner_product_param { num_output: 512 weight_filler: { type: 'gaussian' std: 0.01 } } }
layer { name: 'relu_cls_1' type: 'ReLU' bottom: 'cls_1' top: 'cls_1' }
layer { name: 'dropout_cls_1' type: 'Dropout' bottom: 'cls_1' top: 'cls_1' dropout_param { dropout_ratio: 0.5 } }


layer { name: 'cls_2' type: 'InnerProduct' bottom: 'cls_1' top: 'cls_2'
  param { lr_mult: 1.0 } param { lr_mult: 1.0 }
  inner_product_param { num_output: 1024 weight_filler: { type: 'gaussian' std: 0.01 } } }
layer { name: 'relu_cls_2' type: 'ReLU' bottom: 'cls_2' top: 'cls_2' }

########## cls score ##########

layer { name: 'obj_score' type: 'InnerProduct' bottom: 'cls_2' top: 'obj_score' 
  inner_product_param { num_output: 1 weight_filler { type: "gaussian" std: 0.001 } bias_filler { type: "constant" std: 0 } } }
layer { name: "obj_reshape" type: "Reshape" bottom: "obj_score" top: "obj_score_reshape" 
  reshape_param { shape { dim: -1 dim: 1 dim: 1 dim: 1 } } }

layer { name: 'sig_score' type: 'Sigmoid' bottom: 'obj_score' top: 'objn' }

layer { name: "top_k" type: "DummyData" top: "k" dummy_data_param { shape { dim: 1000 dim: 1 dim: 1 dim: 1} } }

layer { name: "batch_filter" type: "TopKOld" bottom: "sample_s" bottom: "objn" bottom: "k" top: "filted_sample" top: "top_k" }



########## att branch ##########

layer { name: 'att' type: 'InnerProduct' bottom: 'filted_sample' top: 'flatten_att'
  param { lr_mult: 1.0 } param { lr_mult: 2.0 }
  inner_product_param { num_output: 100 weight_filler: { type: 'gaussian' std: 0.0001 } bias_filler { type: 'constant' value: 0 } } }

layer {
  name: "atts_reshape" type: "Reshape" bottom: "flatten_att" top: "atts"
  reshape_param { shape { dim: -1 dim: 1 dim: 10, dim: 10 } }
}

######### att filter ###########

layer { name: 'att_sigmoid' type: 'Sigmoid' bottom: 'atts' top: 'sig_atts' }
layer { name: 'att_filt' type: 'TileProduct' bottom: 'filted_sample' bottom: 'sig_atts' top: 'atts_filt_feat' }

########## seg branch ##########

layer { name: 'seg_1' type: 'InnerProduct' bottom: 'atts_filt_feat' top: 'seg_1'
    param { lr_mult: 1.0 }
    inner_product_param { num_output: 512 weight_filler: { type: 'gaussian' std: 0.001 } bias_term: false } } 

layer { bottom: "seg_1" top: "seg_1" type: "Power" name:"seg_1_power" power_param { scale: 316.22776601684}  }
layer { bottom: "seg_1" top: "seg_1" type: "Scale" name: "seg_1_scale" scale_param { bias_term: true } 
  param { lr_mult: 1 decay_mult: 0 } param { lr_mult: 1 decay_mult: 0} }

layer { name: 'seg_2' type: 'InnerProduct' bottom: 'seg_1' top: 'seg_2'
  param { lr_mult: 1.0 } param { lr_mult: 2.0 }
  inner_product_param { num_output: 1600 weight_filler: { type: 'gaussian' std: 0.001 } bias_filler { type: 'constant' value: 0 } } }

########## seg score ##########

layer { name: 'mask_reshape1' type: 'Reshape' bottom: 'seg_2' top: 'seg_mask'
  reshape_param { shape { dim: 1 dim: -1 dim: 40 dim: 40 } } }

layer { name: 'upscore_' type: "Deconvolution" bottom: "seg_mask" top: "seg_mask_up" param { lr_mult: 0 }
  convolution_param { num_output: 1000 group: 1000 bias_term: false kernel_size: 8 stride: 4 } }

layer { name: "mask_shape" type: "DummyData" top: "mask_shape" dummy_data_param { shape { dim: 1 dim: 1 dim: 160 dim: 160 } } }

layer { name: "crop" type: "Crop" bottom: "seg_mask_up" bottom: "mask_shape" top: "seg_mask_crop" crop_param { axis: 2 offset: 2 } }

layer { name: "mask_reshap2" type: "Reshape" bottom: "seg_mask_crop" top: "seg_mask_crop_reshape" 
  reshape_param { shape { dim: -1 dim: 1 dim: 160 dim: 160 } } }

layer { name: 'seg' type: 'Sigmoid' bottom: 'seg_mask_crop_reshape' top: 'masks' }

#################################
########### SPX #################
#################################

########## sample segmentations ##########

layer {
  name: "scaleFlags_8"
  type: "ScaleFlags"
  bottom: "conv_feat_1_8s"
  bottom: "seg_8"
  bottom: "obj8_flags"
  top: "seg8_flags"
  scale_flags_param{ scalefactor: 4 } #w.r.t. segmentation
}

layer {
  name: "scaleFlags_16"
  type: "ScaleFlags"
  bottom: "conv_feat_1_16s"
  bottom: "seg_16"
  bottom: "obj16_flags"
  top: "seg16_flags"
  scale_flags_param{ scalefactor: 8 }
}

layer {
  name: "scaleFlags_24"
  type: "ScaleFlags"
  bottom: "conv_feat_1_24s"
  bottom: "seg_24"
  bottom: "obj24_flags"
  top: "seg24_flags"
  scale_flags_param{ scalefactor: 12 }
}

layer {
  name: "scaleFlags_32"
  type: "ScaleFlags"
  bottom: "conv_feat_1_32s"
  bottom: "seg_32"
  bottom: "obj32_flags"
  top: "seg32_flags"
  scale_flags_param{ scalefactor: 16 }
}

layer {
  name: "scaleFlags_48"
  type: "ScaleFlags"
  bottom: "conv_feat_1_48s"
  bottom: "seg_48"
  bottom: "obj48_flags"
  top: "seg48_flags"
  scale_flags_param{ scalefactor: 24 }
}

layer {
  name: "scaleFlags_64"
  type: "ScaleFlags"
  bottom: "conv_feat_1_64s"
  bottom: "seg_64"
  bottom: "obj64_flags"
  top: "seg64_flags"
  scale_flags_param{ scalefactor: 32 }
}

layer {
  name: "scaleFlags_96"
  type: "ScaleFlags"
  bottom: "conv_feat_1_96s"
  bottom: "seg_96"
  bottom: "obj96_flags"
  top: "seg96_flags"
  scale_flags_param{ scalefactor: 48 }
}

layer {
  name: "scaleFlags_128"
  type: "ScaleFlags"
  bottom: "conv_feat_1_128s"
  bottom: "seg_128"
  bottom: "obj128_flags"
  top: "seg128_flags"
  scale_flags_param{ scalefactor: 64 }
}

layer {
  name: "concatFlattendNonObj"
  bottom: "seg8_flags"
  bottom: "seg16_flags"
  bottom: "seg24_flags"
  bottom: "seg32_flags"
  bottom: "seg48_flags"
  bottom: "seg64_flags"
  bottom: "seg96_flags"
  bottom: "seg128_flags"
  top: "seg_flags"
  type: "Concat"
  concat_param {
    axis: -1
  }
}

layer {
  name: "selectSegFlags"
  type: "Python"
  bottom: "seg_flags"
  bottom: "top_k"
  top: 'segFlagsPos'
  python_param {
    module: "data.selectSegFlagsLayer"
    layer: "SelectSegFlagsLayer"
  }
}

layer{
  type: "Python"
  bottom: "segFlagsPos"
  bottom: "seg8_flags"
  bottom: "seg16_flags"
  bottom: "seg24_flags"
  bottom: "seg32_flags"
  bottom: "seg48_flags"
  bottom: "seg64_flags"
  bottom: "seg96_flags"
  bottom: "seg128_flags"
  top: "seg8_flagsPos"
  top: "seg16_flagsPos"
  top: "seg24_flagsPos"
  top: "seg32_flagsPos"
  top: "seg48_flagsPos"
  top: "seg64_flagsPos"
  top: "seg96_flagsPos"
  top: "seg128_flagsPos"
  top: "seg8_flagsPosChecked"
  top: "seg16_flagsPosChecked"
  top: "seg24_flagsPosChecked"
  top: "seg32_flagsPosChecked"
  top: "seg48_flagsPosChecked"
  top: "seg64_flagsPosChecked"
  top: "seg96_flagsPosChecked"
  top: "seg128_flagsPosChecked"
python_param {
    module: "data.splitSegFlagsLayer"
    layer: "SplitSegFlags"
  }
}



layer { name: "seg_extractor_8" type: "SlidingWindowIndexSeg" bottom: "seg_8" bottom: "seg8_flagsPosChecked" propagate_down: true propagate_down: false top: "pooledSeg_8"
  sliding_window_param { window_h: 40 window_w: 40 }}

layer { name: "seg_extractor_16" type: "SlidingWindowIndexSeg" bottom: "seg_16" bottom: "seg16_flagsPosChecked" propagate_down: true propagate_down: false top: "pooledSeg_16"
  sliding_window_param { window_h: 80 window_w: 80 }}

layer { name: "seg_extractor_24" type: "SlidingWindowIndexSeg" bottom: "seg_24" bottom: "seg24_flagsPosChecked" propagate_down: true propagate_down: false top: "pooledSeg_24"
  sliding_window_param { window_h: 120 window_w: 120 }}

layer { name: "seg_extractor_32" type: "SlidingWindowIndexSeg" bottom: "seg_32" bottom: "seg32_flagsPosChecked" propagate_down: true propagate_down: false top: "pooledSeg_32"
  sliding_window_param { window_h: 160 window_w: 160 }}

layer { name: "seg_extractor_48" type: "SlidingWindowIndexSeg" bottom: "seg_48" bottom: "seg48_flagsPosChecked" propagate_down: true propagate_down: false top: "pooledSeg_48"
  sliding_window_param { window_h: 240 window_w: 240 }}

layer { name: "seg_extractor_64" type: "SlidingWindowIndexSeg" bottom: "seg_64" bottom: "seg64_flagsPosChecked" propagate_down: true propagate_down: false top: "pooledSeg_64"
  sliding_window_param { window_h: 320 window_w: 320 }}

layer { name: "seg_extractor_96" type: "SlidingWindowIndexSeg" bottom: "seg_96" bottom: "seg96_flagsPosChecked" propagate_down: true propagate_down: false top: "pooledSeg_96"
  sliding_window_param { window_h: 480 window_w: 480 }}

layer { name: "seg_extractor_128" type: "SlidingWindowIndexSeg" bottom: "seg_128" bottom: "seg128_flagsPosChecked" propagate_down: true propagate_down: false top: "pooledSeg_128"
  sliding_window_param { window_h: 640 window_w: 640 }}

########## splitMasks ##########

layer { name: 'mask_reshape' type: 'Reshape' bottom: 'seg_2' top: 'seg_mask_spx'
  reshape_param { shape { dim: -1 dim: 1 dim: 40 dim: 40 } } }


layer { name: 'upscore' type: "Deconvolution" bottom: "seg_mask_spx" top: "seg_mask_up_spx" param { lr_mult: 0 }
  convolution_param { num_output: 1 bias_term: false kernel_size: 8 stride: 4 pad: 2 } }


layer { name: 'seg' type: 'Sigmoid' bottom: 'seg_mask_up_spx' top: 'seg_mask_sig' }

layer {
  name: "print"
  type: "Python"
  bottom: 'seg_mask_sig' bottom: 'seg8_flagsPos' bottom: 'seg16_flagsPos' bottom: 'seg24_flagsPos' bottom: 'seg32_flagsPos' bottom: 'seg48_flagsPos' bottom: 'seg64_flagsPos' bottom: 'seg96_flagsPos' bottom: 'seg128_flagsPos',   bottom: "segFlagsPos"
  top: 'segMask_8' top: 'segMask_16' top: 'segMask_24' top: 'segMask_32' top: 'segMask_48' top: 'segMask_64' top: 'segMask_96' top: 'segMask_128'
  python_param {
    module: "data.segMasksTestLayer"
    layer: "SegMasksTestLayer"
  }
}

########## spx feature extraction #############

layer {  name: "featExtractor_spx_8"  type: "Convolution"  bottom: "ResNetnSequentialnlayer1nnBasicBlockn2n206"  top: "feat_spx_8"
  param {   lr_mult: 10    decay_mult: 1  }  
  convolution_param {  num_output: 256  pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_term: true  bias_filler { type: "constant" std: 0 }   }
}
layer { name: 'featExtractor_spx_relu_8' type: 'ReLU' bottom: 'feat_spx_8' top: 'feat_spx_8' }

layer {  name: "featExtractor_spx_16"  type: "Convolution"  bottom: "ResNetnSequentialnlayer1nnBasicBlockn2n206"  top: "feat_spx_16"
  param {  lr_mult: 10    decay_mult: 1  }  
  convolution_param {  num_output: 256  pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_term: true  bias_filler { type: "constant" std: 0 }  }
}
layer { name: 'featExtractor_spx_relu_16' type: 'ReLU' bottom: 'feat_spx_16' top: 'feat_spx_16' }

layer {  name: "featExtractor_spx_32"  type: "Convolution"  bottom: "ResNetnSequentialnlayer1nnBasicBlockn2n206"  top: "feat_spx_32"
  param { lr_mult: 10    decay_mult: 1  }  
  convolution_param {  num_output: 256  pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_term: true  bias_filler { type: "constant" std: 0 }   }
}
layer { name: 'featExtractor_spx_relu_32' type: 'ReLU' bottom: 'feat_spx_32' top: 'feat_spx_32' }

layer {  name: "featExtractor_spx_64"  type: "Convolution"  bottom: "ResNetnSequentialnlayer1nnBasicBlockn2n206"  top: "feat_spx_64"
  param { lr_mult: 10    decay_mult: 1  }  
  convolution_param {  num_output: 256  pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }    bias_term: true  bias_filler { type: "constant" std: 0 }   }
}
layer { name: 'featExtractor_spx_relu_64' type: 'ReLU' bottom: 'feat_spx_64' top: 'feat_spx_64' }

layer {  name: "featExtractor_spx_128"  type: "Convolution"  bottom: "ResNetnSequentialnlayer1nnBasicBlockn2n206"  top: "feat_spx_128"
  param {  lr_mult: 10    decay_mult: 1  }  
  convolution_param {  num_output: 256  pad: 0    kernel_size: 1    stride: 1  weight_filler {      type: "gaussian"      std: 0.01    }   bias_term: true  bias_filler { type: "constant" std: 0 }   }
}
layer { name: 'featExtractor_spx_relu_128' type: 'ReLU' bottom: 'feat_spx_128' top: 'feat_spx_128' }



########## attPooling #############

layer {
  name: "segMask8_down"
  type: "Pooling"
  bottom: "segMask_8"
  top: "segMask_8_down"
  pooling_param {
    pool: AVE
    kernel_size: 4
    stride: 4
  }
}

layer {
  name: "attSpxPooling_8_GPU"
  type: "AttSpxPooling"
  bottom: "segMask_8_down"
  bottom: "pooledSeg_8"
  bottom: "seg_8"
  top: "attPerSpx_8"
  top: "pxCount_8_att"
  superpixel_pooling_param {
    pool: AVE
    weight: 1
  }
}

layer {
  name: "segMask16_down"
  type: "Pooling"
  bottom: "segMask_16"
  top: "segMask_16_down"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}

layer {
  name: "attSpxPooling_16_GPU"
  type: "AttSpxPooling"
  bottom: "segMask_16_down"
  bottom: "pooledSeg_16"
  bottom: "seg_16"
  top: "attPerSpx_16"
  top: "pxCount_16_att"
  superpixel_pooling_param {
    pool: AVE
    weight: 1
  }
}


layer {
  name: "segMask24_down"
  type: "Pooling"
  bottom: "segMask_24"
  top: "segMask_24_down"
  pooling_param {
    pool: AVE
    kernel_size: 4
    stride: 4
  }
}

layer { name: 'att_up_24' type: "Deconvolution" bottom: "segMask_24_down" top: "att_up_24" param { lr_mult: 0 } #4x
  convolution_param { num_output: 1 bias_term: false kernel_size: 6 stride: 3 pad: 1 } }

layer {
  name: 'cut24'
  type: "Cut"
  bottom: "att_up_24"
  top: "att_up_24_cut"
}

layer {
  name: "attSpxPooling_24_GPU"
  type: "AttSpxPooling"
  bottom: "att_up_24_cut"
  bottom: "pooledSeg_24"
  bottom: "seg_24"
  top: "attPerSpx_24"
  top: "pxCount_24_att"
  superpixel_pooling_param {
    pool: AVE
    weight: 1
  }
}

layer {
  name: "attSpxPooling_32_GPU"
  type: "AttSpxPooling"
  bottom: "segMask_32"
  bottom: "pooledSeg_32"
  bottom: "seg_32"
  top: "attPerSpx_32"
  top: "pxCount_32_att"
  superpixel_pooling_param {
    pool: AVE
    weight: 1
  }
}

layer {
  name: "segMask48_down"
  type: "Pooling"
  bottom: "segMask_48"
  top: "segMask_48_down"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}

layer { name: 'att_up_48' type: "Deconvolution" bottom: "segMask_48_down" top: "att_up_48" param { lr_mult: 0 } #4x
  convolution_param { num_output: 1 bias_term: false kernel_size: 6 stride: 3 pad: 1 } }

layer {
  name: 'cut48'
  type: "Cut"
  bottom: "att_up_48"
  top: "att_up_48_cut"
}

layer {
  name: "attSpxPooling_48_GPU"
  type: "AttSpxPooling"
  bottom: "att_up_48_cut"
  bottom: "pooledSeg_48"
  bottom: "seg_48"
  top: "attPerSpx_48"
  top: "pxCount_48_att"
  superpixel_pooling_param {
    pool: AVE
    weight: 1
  }
}

layer { name: 'att_up_64' type: "Deconvolution" bottom: "segMask_64" top: "att_up_64" param { lr_mult: 0 } #4x
  convolution_param { num_output: 1 bias_term: false kernel_size: 4 stride: 2 pad: 1 } }

layer {
  name: "attSpxPooling_64_GPU"
  type: "AttSpxPooling"
  bottom: "att_up_64"
  bottom: "pooledSeg_64"
  bottom: "seg_64"
  top: "attPerSpx_64"
  top: "pxCount_64_att"
  superpixel_pooling_param {
    pool: AVE
    weight: 1
  }
}

layer { name: 'att_up_96' type: "Deconvolution" bottom: "segMask_96" top: "att_up_96" param { lr_mult: 0 } #4x
  convolution_param { num_output: 1 bias_term: false kernel_size: 6 stride: 3 pad: 1 } }

layer {
  name: 'cut96'
  type: "Cut"
  bottom: "att_up_96"
  top: "att_up_96_cut"
}

layer {
  name: "attSpxPooling_96_GPU"
  type: "AttSpxPooling"
  bottom: "att_up_96_cut"
  bottom: "pooledSeg_96"
  bottom: "seg_96"
  top: "attPerSpx_96"
  top: "pxCount_96_att"
  superpixel_pooling_param {
    pool: AVE
    weight: 1
  }
}

layer { name: 'att_up_128' type: "Deconvolution" bottom: "segMask_128" top: "att_up_128" param { lr_mult: 0 } #4x
  convolution_param { num_output: 1 bias_term: false kernel_size: 8 stride: 4 pad: 2 } }

layer {
  name: "attSpxPooling_128_GPU"
  type: "AttSpxPooling"
  bottom: "att_up_128"
  bottom: "pooledSeg_128"
  bottom: "seg_128"
  top: "attPerSpx_128"
  top: "pxCount_128_att"
  superpixel_pooling_param {
    pool: AVE
    weight: 1
  }
}


layer {
  name: "silence"
  type: "Silence"
  bottom: "pxCount_128_att"
  bottom: "pxCount_64_att"
  bottom: "pxCount_32_att"
  bottom: "pxCount_16_att"
  bottom: "pxCount_8_att"
}

########## FEATURE SPX POOLING #############

layer {
  name: "pool8"
  type: "SpxPooling"
  bottom: "feat_spx_8"
  bottom: "seg_8"
  top: "pooledSpxs_8"
  top: "pxCount_8"
  superpixel_pooling_param {
    pool: AVE
    weight: 1
  }
}

layer {
  name: "pool16"
  type: "SpxPooling"
  bottom: "feat_spx_16"
  bottom: "seg_16"
  top: "pooledSpxs_16"
  top: "pxCount_16"
  superpixel_pooling_param {
    pool: AVE
    weight: 1
  }
}

layer {
  name: "pool24"
  type: "SpxPooling"
  bottom: "feat_spx_16"
  bottom: "seg_24"
  top: "pooledSpxs_24"
  top: "pxCount_24"
  superpixel_pooling_param {
    pool: AVE
    weight: 1
  }
}

layer {
  name: "pool32"
  type: "SpxPooling"
  bottom: "feat_spx_32"
  bottom: "seg_32"
  top: "pooledSpxs_32"
  top: "pxCount_32"
  superpixel_pooling_param {
    pool: AVE
    weight: 1
  }
}

layer {
  name: "pool48"
  type: "SpxPooling"
  bottom: "feat_spx_32"
  bottom: "seg_48"
  top: "pooledSpxs_48"
  top: "pxCount_48"
  superpixel_pooling_param {
    pool: AVE
    weight: 1
  }
}

layer {
  name: "pool64"
  type: "SpxPooling"
  bottom: "feat_spx_64"
  bottom: "seg_64"
  top: "pooledSpxs_64"
  top: "pxCount_64"
  superpixel_pooling_param {
    pool: AVE
    weight: 1
  }
}

layer {
  name: "pool96"
  type: "SpxPooling"
  bottom: "feat_spx_64"
  bottom: "seg_96"
  top: "pooledSpxs_96"
  top: "pxCount_96"
  superpixel_pooling_param {
    pool: AVE
    weight: 1
  }
}

layer {
  name: "pool128"
  type: "SpxPooling"
  bottom: "feat_spx_128"
  bottom: "seg_128"
  top: "pooledSpxs_128"
  top: "pxCount_128"
  superpixel_pooling_param {
    pool: AVE
    weight: 1
  }
}

layer {
  name: "SilencePxCount"
  type: "Silence"
  bottom: "pxCount_8"
  bottom: "pxCount_16"
  bottom: "pxCount_32"
  bottom: "pxCount_64"
  bottom: "pxCount_128"
}

############ SPX SAMPLING ############

layer {
  name: "spxSampling_8"
  type: "Python"
  bottom: "pooledSpxs_8" 
  bottom: "attPerSpx_8"
  bottom: "pooledSeg_8"
  top: "sampledSpx_8"
  top: "sampledSpxInfos_8"
  python_param {
    module: "data.spxSamplingLayerTest"
    layer: "SpxSamplingLayer"
    param_str: '{"scale": 8}'
  }
propagate_down: True
propagate_down: False
propagate_down: False
}

layer {
  name: "spxSampling_16"
  type: "Python"
  bottom: "pooledSpxs_16" 
  bottom: "attPerSpx_16"
  bottom: "pooledSeg_16"
  top: "sampledSpx_16"
  top: "sampledSpxInfos_16"
  python_param {
    module: "data.spxSamplingLayerTest"
    layer: "SpxSamplingLayer"
    param_str: '{"scale": 16}'
  }
propagate_down: True
propagate_down: False
propagate_down: False
}

layer {
  name: "spxSampling_24"
  type: "Python"
  bottom: "pooledSpxs_24" 
  bottom: "attPerSpx_24"
  bottom: "pooledSeg_24"
  top: "sampledSpx_24"
  top: "sampledSpxInfos_24"
  python_param {
    module: "data.spxSamplingLayerTest"
    layer: "SpxSamplingLayer"
    param_str: '{"scale": 24}'
  }
propagate_down: True
propagate_down: False
propagate_down: False
}

layer {
  name: "spxSampling_32"
  type: "Python"
  bottom: "pooledSpxs_32" 
  bottom: "attPerSpx_32"
  bottom: "pooledSeg_32"
  top: "sampledSpx_32"
  top: "sampledSpxInfos_32"
  python_param {
    module: "data.spxSamplingLayerTest"
    layer: "SpxSamplingLayer"
    param_str: '{"scale": 32}'
  }
propagate_down: True
propagate_down: False
propagate_down: False
}

layer {
  name: "spxSampling_48"
  type: "Python"
  bottom: "pooledSpxs_48" 
  bottom: "attPerSpx_48"
  bottom: "pooledSeg_48"
  top: "sampledSpx_48"
  top: "sampledSpxInfos_48"
  python_param {
    module: "data.spxSamplingLayerTest"
    layer: "SpxSamplingLayer"
    param_str: '{"scale": 48}'
  }
propagate_down: True
propagate_down: False
propagate_down: False
}

layer {
  name: "spxSampling_64"
  type: "Python"
  bottom: "pooledSpxs_64" 
  bottom: "attPerSpx_64"
  bottom: "pooledSeg_64"
  top: "sampledSpx_64"
  top: "sampledSpxInfos_64"
  python_param {
    module: "data.spxSamplingLayerTest"
    layer: "SpxSamplingLayer"
    param_str: '{"scale": 64}'
  }
propagate_down: True
propagate_down: False
propagate_down: False
}

layer {
  name: "spxSampling_96"
  type: "Python"
  bottom: "pooledSpxs_96" 
  bottom: "attPerSpx_96"
  bottom: "pooledSeg_96"
  top: "sampledSpx_96"
  top: "sampledSpxInfos_96"
  python_param {
    module: "data.spxSamplingLayerTest"
    layer: "SpxSamplingLayer"
    param_str: '{"scale": 96}'
  }
propagate_down: True
propagate_down: False
propagate_down: False
}

layer {
  name: "spxSampling_128"
  type: "Python"
  bottom: "pooledSpxs_128" 
  bottom: "attPerSpx_128"
  bottom: "pooledSeg_128"
  top: "sampledSpx_128"
  top: "sampledSpxInfos_128"
  python_param {
    module: "data.spxSamplingLayerTest"
    layer: "SpxSamplingLayer"
    param_str: '{"scale": 128}'
  }
propagate_down: True
propagate_down: False
propagate_down: False
}

############ SPX CLASSIFICATION ############

layer { name: "spxPairs_concat" type: "Concat"
  bottom: "sampledSpx_8" bottom: "sampledSpx_16" bottom: "sampledSpx_24" bottom: "sampledSpx_32" bottom: "sampledSpx_48" bottom: "sampledSpx_64" bottom: "sampledSpx_96" bottom: "sampledSpx_128"
top: "batchSpxPairs"
  concat_param { concat_dim: 0 } }

layer { bottom: "batchSpxPairs" top: "batchSpxPairs" type: "BatchNorm" name:"spx_bn" batch_norm_param { use_global_stats: false }
  param { lr_mult: 0 decay_mult: 0 } param { lr_mult: 0 decay_mult: 0 } param { lr_mult: 0 decay_mult: 0 } }
layer { bottom: "batchSpxPairs" top: "batchSpxPairs" type: "Scale" name: "spx_scale" scale_param { bias_term: true }
  param { lr_mult: 10 decay_mult: 0 } param { lr_mult: 20 decay_mult: 0} }


layer { name: 'spx_1' type: 'InnerProduct' bottom: 'batchSpxPairs' top: 'spx_1'
    param { lr_mult: 10.0 } param { lr_mult: 20.0 }
    inner_product_param { num_output: 512 weight_filler: { type: 'gaussian' std: 0.01 } } }
layer { name: 'relu_spx_1' type: 'ReLU' bottom: 'spx_1' top: 'spx_1' }

layer { name: 'spx_2' type: 'InnerProduct' bottom: 'spx_1' top: 'spx_2'
  param { lr_mult: 10.0 } param { lr_mult: 20.0 }
  inner_product_param { num_output: 512 weight_filler: { type: 'gaussian' std: 0.01 } } }
layer { name: 'relu_spx_2' type: 'ReLU' bottom: 'spx_2' top: 'spx_2' }

layer { name: 'spx_3' type: 'InnerProduct' bottom: 'spx_2' top: 'spx_3'
  param { lr_mult: 10.0 } param { lr_mult: 20.0 }
  inner_product_param { num_output: 512 weight_filler: { type: 'gaussian' std: 0.01 } } }
layer { name: 'relu_spx_3' type: 'ReLU' bottom: 'spx_3' top: 'spx_3' }

layer { name: 'spx_score' type: 'InnerProduct' bottom: 'spx_3' top: 'spx_score' 
  param { lr_mult: 10.0 } param { lr_mult: 20.0 }
  inner_product_param { num_output: 1 weight_filler { type: "gaussian" std: 0.001 } bias_filler { type: "constant" std: 0 } } }
layer { name: "spx_reshape" type: "Reshape" bottom: "spx_score" top: "spx_score_reshape" 
  reshape_param { shape { dim: -1 dim: 1 dim: 1 dim: 1 } } }


layer { name: 'seg' type: 'Sigmoid' bottom: 'spx_score_reshape' top: 'spx_score_sig' }

############ SPX INFOS ############

layer { name: "spxInfos_concat" type: "Concat"
  bottom: "sampledSpxInfos_8" bottom: "sampledSpxInfos_16" bottom: "sampledSpxInfos_24" bottom: "sampledSpxInfos_32" bottom: "sampledSpxInfos_48" bottom: "sampledSpxInfos_64" bottom: "sampledSpxInfos_96" bottom: "sampledSpxInfos_128"
top: "batchSpxInfos"
  concat_param { concat_dim: 0 } }

layer {
  name: "silence"
  type: "Silence"
  bottom: "obj_score_reshape"
  bottom: "spx_score_reshape"
  bottom: "batchSpxInfos"
}

